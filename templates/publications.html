
<div class="row">

	<h2>Publications</h2>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
	<tr>

            <td width="30%"><img src="static/img/squint_teaser.jpg" alt="3DSP" width="300" height="auto" style="border-style: none"></td>
            <td valign="top" width="70%">
            <papertitle>SQuINTing at VQA Models: Interrogating VQA Models with Sub-Questions</papertitle>
            <br>
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
            <a href="http://purvaten.github.io">Purva Tendulkar</a>,
	    <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
            <a href="https://www.microsoft.com/en-us/research/people/">Eric Horvitz</a>,
            <a href="https://www.microsoft.com/en-us/research/people/marcotcr/">Marco Tulio Ribeiro</a>,
            <a href="https://www.microsoft.com/en-us/research/people/benushi">Besmira Nushi</a>,
            <a href="https://www.microsoft.com/en-us/research/people/eckamar/">Ece Kamar</a>
            <br>
              <em>Under review</em><br>
              <abstract>We investigate the capabilities of VQA models for solving tasks that differ in nature and in complexity. We notice that existing VQA models have consistency issues -- they answer complex reasoning question correctly but fail on associated low-level perception sub-questions. We quantify the extent to which this phenomenon occurs by creating a new Reasoning split and collecting Sub-VQA, a new dataset consisting of associated perception sub-questions needed to effectively answer the main reasoning question. Additionally, we propose SQuINT approach which enforces models be right for the right reasons.</abstract> 
            <br>
            <br>

            </td>
	</tr>

    <tr>
            <td width="30%"><img src="static/img/gradcam_teaser2.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://link.springer.com/article/10.1007/s11263-019-01228-7">
                      <papertitle>Visual Explanations from Deep Networks</papertitle>
              </a>
            <br>
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="mcogswell.io">Michael Cogswell</a>,  
      <a href="http://abhishekdas.com/about/">Abhishek Das</a>, 
			<a href="http://ramakrishnavedantam928.github.io/">Ramakrishna Vedantam</a>, 
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>
            <br>
              <conference>IJCV, 2019</conference><br>
              <a href="https://arxiv.org/abs/1610.02391">arxiv</a> /
              <a href="https://mlatgt.blog/2018/09/05/choose-your-neuron-incorporating-domain-knowledge-through-neuron-importance/">blogpost</a> /
              <a href="https://github.com/ramprs/grad-cam">code</a> /
              <a href="http://gradcam.cloudcv.org/">demo</a>
            <br>
            <br>
            </td>
	</tr>
	<tr>
            <td width="30%"><img src="static/img/hint_teaser.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1902.03751">
                      <papertitle>Taking a HINT: Levaraging Explanations to Make Vision and Language Models More Grounded</papertitle>
              </a>
            <br>
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="https://www.cc.gatech.edu/~slee3191/">Stefen Lee</a>,
			<a href="">Yilin Shen</a>, 
			<a href="">Hongxia Jin</a>,
			<a href="">Shalini Ghosh</a>,
			<a href="">Larry Heck</a>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, 
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>
            <br>
              <conference>ICCV, 2019</conference><br>
                <a href="https://arxiv.org/abs/1902.03751">arxiv</a> /
                <a href="https://mlatgt.blog/2019/10/02/taking-a-hint-leveraging-explanations-to-make-vision-and-language-models-more-grounded/">blogpost</a>

            <br>
              <abstract>We notice that many vision and language models suffer from poor visual grounding - often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. To tackle this, we propose Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. We show that encouraging these models to look at same regions as humans makes them generalize to new distributions better. </abstract>
            <br>
            <br>
            </td>
	</tr>



    <tr>
            <td width="30%"><img src="static/img/treat_teaser.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1903.07820">
                      <papertitle>Trick or Treat: Thematic Reinforcement for Artistic Typography</papertitle>
              </a>
            <br>
            <a href="https://purvaten.github.io">Purva Tendulkar</a>,
            <a href="http://martiansideofthemoon.github.io/">Kalpesh Krishna</a>,
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>
            <br>
              <conference>ICCC, 2019</conference><br>
		<a href="https://arxiv.org/abs/1903.07820">arxiv</a> /
              <a href="https://github.com/purvaten/treat">code</a> /
              <a href="https://doodle.cloudcv.org">demo</a>

            <br>
              <abstract>We propose an approach to make text visually appealing and memorable is semantic reinforcement - the use of visual cues alluding to the context or theme in which the word is being used to reinforce the message (e.g., Google Doodles). Given an input word (e.g. exam) and a theme (e.g. education), the individual letters of the input word are replaced by cliparts relevant to the theme which visually resemble the letters - adding creative context to the potentially boring input word. </abstract> 
            <br>
            <br>
            </td>
	</tr>


        <tr>
            <td width="30%"><img src="static/img/niwt_teaser.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1808.02861">
                      <papertitle>Choose Your Neuron: Incorporating Domain Knowledge into Deep Networks through Neuron Importance</papertitle>
              </a>
            <br>
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju*</b>,
			<a href="https://about.me/chattopadhyayprithvijit">Prithvijit Chattopadhyay*</a>, 
			<a href="https://sites.google.com/site/mhelhoseiny/">Mohamed Elhoseiny</a>,
			<a href="">Tilak Sharma</a>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
			<a href="https://www.cc.gatech.edu/~slee3191/">Stefen Lee</a>
            <br>
              <conference>ECCV, 2018</conference><br>
              <a href="https://arxiv.org/abs/1808.02861">arxiv</a> /
              <a href="https://mlatgt.blog/2018/09/05/choose-your-neuron-incorporating-domain-knowledge-through-neuron-importance/">blogpost</a> /
              <a href="https://github.com/ramprs/neuron-importance-zsl">code</a>
            <br>
              <abstract>Individual neurons in CNNs implicitly learn semantically meaningful concepts ranging from simple textures and shapes to whole objects. We introduce an efficient zero-shot learning approach that learns to map domain knowledge about novel classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts. We also show how our approach can provide visual and textual explanations including neuron names.</abstract>
            <br>
            <br>
            </td>
	</tr>

    
	<tr>
            <td width="30%"><img src="static/img/dbs_teaser.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1610.02424">
                      <papertitle>Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</papertitle>
              </a>
            <br>
            <a href="https://computing.ece.vt.edu/~ashwinkv/">Ashwin Kalyan</a>, 
			<a href="mcogswell.io">Michael Cogswell</a>, 
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="https://computing.ece.vt.edu/~sunqing/">Qing Sun</a>,
			<a href="https://www.cc.gatech.edu/~slee3191/">Stefen Lee</a>,
			<a href="https://www.cs.indiana.edu/~djcran/">David Crandal</a>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>
            <br>
              <conference>AAAI, 2018</conference><br>
              <a href="https://arxiv.org/abs/1610.02424">arxiv</a> /
              <a href="https://github.com/ramprs/neuron-importance-zsl">code</a> /
              <a href="http://dbs.cloudcv.org/">demo</a>
            <br>
              <abstract>We propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. We study the role of diversity for image-grounded language generation tasks as the complexity of the image changes.</abstract>
            <br>
            <br>
            </td>
	</tr>

    <tr>
            <td width="30%"><img src="static/img/gradcam_teaser1.png" alt="3DSP" width="300" height="auto" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1610.02391">
                      <papertitle>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</papertitle>
              </a>
            <br>
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="mcogswell.io">Michael Cogswell</a>,  
      <a href="http://abhishekdas.com/about/">Abhishek Das</a>, 
			<a href="http://ramakrishnavedantam928.github.io/">Ramakrishna Vedantam</a>, 
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>
            <br>
              <conference>ICCV, 2017</conference><br>
              <a href="https://arxiv.org/abs/1610.02391">arxiv</a> /
              <a href="https://mlatgt.blog/2018/09/05/choose-your-neuron-incorporating-domain-knowledge-through-neuron-importance/">blogpost</a> /
              <a href="https://github.com/ramprs/grad-cam">code</a> /
              <a href="http://gradcam.cloudcv.org/">demo</a>

            <br>
              <abstract>We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We apply Grad-CAM to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' model from a 'weaker' one even when both make identical predictions.</abstract>
            <br>
            <br>
            </td>
	</tr>

	<tr>
            <td width="30%"><img src="static/img/counting_teaser.jpg" alt="3DSP" width="230" height="140" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1604.03005">
                      <papertitle>Counting Everyday Objects in Everyday Scenes</papertitle>
              </a>
            <br>
			<a href="https://about.me/chattopadhyayprithvijit">Prithviraj Chattopadhyay*</a>, 
            Ramakrishna Vedantam<sup>*</sup>,
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
			<a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>
            <br>
              <conference>CVPR, 2017</conference> <font color="red">(Spotlight)</font><br>
              <a href="https://arxiv.org/abs/1604.03005">arxiv</a> /
              <a href="https://github.com/prithv1/cvpr2017_counting">code</a>

            <br>
              <abstract>We study the numerosity of object classes in natural, everyday images and build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. We propose a contextual counting approach inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values.</abstract> 
            <br>
            <br>
</td>
          </tr>

    <tr>
            <td width="30%"><img src="static/img/paintbrush_teaser.jpg" alt="3DSP" width="230" height="140" style="border-style: none"></td>

            <td valign="top" width="70%">
              <a href="https://dl.acm.org/citation.cfm?id=2702222">
                      <papertitle>The Semantic Paintbrush: Interactive 3D Mapping and Recognition in Large Outdoor Spaces</papertitle>
              </a>
            <br>
            <a href="http://www.miksik.co.uk/">Ondrej Miksik</a>,
			<a href="http://stanford.edu/~vibhavv/index.html">Vibhav Vineet</a>,
			<a href="http://morten.lidegaard.net/">Morten Lidegaard</a>,
            <b href="http://ramprs.github.io">Ramprasaath R. Selvaraju</b>,
			<a href="http://graphics.stanford.edu/~niessner/publications.html">Matthias Nießner</a>,
			<a href="http://www.cs.ox.ac.uk/people/stuart.golodetz/">Stuart Golodetz</a>,
			<a href="http://www.ndcn.ox.ac.uk/departments/DCN/team/research-scientists/stephen-hicks">Stephen L. Hicks</a>,
			<a href="http://www.technicolor.com/en/patrick-perez">Patrick Pérez</a>,
			<a href="http://research.microsoft.com/en-us/people/shahrami/">Shahram Izadi</a>,
			<a href="http://www.robots.ox.ac.uk/~tvg/people.php">Philip H. S. Torr</a>
            <br>
            <conference>CHI, 2015</conference> <font color="red">(Oral)</font><br>
              <a href="https://dl.acm.org/ft_gateway.cfm?id=2702222&ftid=1565186&dwn=1&CFID=90041924&CFTOKEN=e40b9eb2e2b8d2a6-DED7AA47-A1B8-63F4-349C0036BF016CBD">paper</a> /
              <a href="https://www.youtube.com/watch?v=3x9qtJjXkuY&feature=emb_logo">video</a>
            <br>
              <abstract>We present an augmented reality system for large scale 3D reconstruction and recognition in outdoor scenes. We use a purely passive stereo setup, allowing for outdoor use. In addition to producing a map of the 3D environment in real-time, it also allows the user to draw (or 'paint') with a laser pointer directly onto the reconstruction to segment the model into objects. </abstract>
            <br>
            <br>
	</td>
          </tr>

    </tbody>

</table>

</div>
